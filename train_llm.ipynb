{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78309612",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers datasets accelerate bitsandbytes trl peft evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6682ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import setup_chat_format, SFTTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=8, #\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\", # [\"q_proj\",\"k_proj\",\"v_proj\"]\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16 #but should be set to the optimal BFloat16 for newer hardware supporting it to achieve the best performance.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2be4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface Model ID\n",
    "model_id = \"Qwen/Qwen2.5-3B\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f58bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  load_dataset(\"mertbozkurt/llama2-TR-recipe\", split=\"train\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88eec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned = re.sub(r\"[\\[\\]/\\\\]\", \"\", text)\n",
    "    cleaned = cleaned.strip()\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd0458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_input_output(example):\n",
    "    text = example[\"text\"]\n",
    "    parts = text.split(\"INST\")\n",
    "    if len(parts) == 3:\n",
    "        input_part = clean_text(parts[1])\n",
    "        output_part = clean_text(parts[2])\n",
    "        return {\"input\": input_part, \"output\": output_part}\n",
    "    else:\n",
    "        return {\"input\": \"\", \"output\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a6587",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = dataset.map(parse_input_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4199df",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = new_dataset.remove_columns(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2510adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5608945",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"Sen insanların istediği tariflere cevap veren bir Aşçısın. Insanların istediklerine göre soruları cevapla.\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message},\n",
    "      {\"role\": \"user\", \"content\": sample[\"input\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"output\"]}\n",
    "    ]\n",
    "  }\n",
    "\n",
    "\n",
    "dataset = dataset.train_test_split(test_size = 0.05)\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset[\"train\"].features,batched=False)\n",
    "\n",
    "print(\"Dataset: \",dataset[\"train\"][0][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c64e079",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"recipe-chat-bot\", # directory to save and repository id\n",
    "    num_train_epochs=5,                     # number of training epochs\n",
    "    per_device_train_batch_size=2,          # batch size per device during training\n",
    "    #gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    #gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "#   push_to_hub=True,                       # push model to hub\n",
    "#   report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
